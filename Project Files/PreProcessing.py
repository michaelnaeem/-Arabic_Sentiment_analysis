#!/usr/bin/env python
# coding: utf-8

# # Preprocessing Stage
# ## Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
# Ù‡Ù†Ø§ Ù‡Ù†Ø³ØªØ¯Ø¹ÙŠ ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„ÙŠ Ø¨Ù†Ø­ØªØ§Ø¬Ù‡Ø§ Ø®Ù„Ø§Ù„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

# In[1]:


import warnings
warnings.filterwarnings("ignore")
from pyarabic import araby
import os
from nltk.tag import StanfordPOSTagger
from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer
import re
from nltk import ISRIStemmer
import pandas as pd
import numpy as np
from wordcloud import WordCloud
import arabic_reshaper
from bidi.algorithm import get_display
import matplotlib.pyplot as plt
import time


# ## 1- Tokenization Function
# Ø¯ÙŠ Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ù„ÙŠ Ø¨ØªÙ‚Ø³Ù… Ø§Ù„Ø¬Ù…Ù„Ø© Ù„ÙƒÙ„Ù…Ø§Øª <br>
# **Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ araby Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**

# In[ ]:


def Tokening(sample):
    tokens = araby.tokenize(sample)
    return tokens


# ğŸ‘‡Ù…Ø«Ø§Ù„: Ù‡Ù†Ø§ Ù…Ø«Ù„Ø§ Ø¨Ù†Ø¯ÙŠÙ„Ù‡ Ø¬Ù…Ù„Ø© ÙˆÙ‡Ùˆ Ø¨ÙŠÙ‚Ø·Ø¹Ù‡Ø§ ÙˆÙŠØ³Ù…ÙŠÙ‡Ø§ ØªÙˆÙƒÙŠÙ†Ø²

# ## 2- Part Of Speech Handling
# Ø¯ÙŠ Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ù„ÙŠ Ø¨ØªØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§ÙŠ Ø§Ø³Ù… Ø¹Ù„Ù… Ø§Ùˆ Ø­Ø±Ù Ø¬Ø± Ø§Ùˆ ÙƒÙ„Ù…Ø© ØºØ±ÙŠØ¨Ù‡ ÙˆØªØ­Ø°ÙÙ‡Ù… Ù…Ù† Ø§Ù„Ø¬Ù…Ù„Ø© ÙˆØªØ®Ù„ÙŠ Ø¨Ø³ Ø§Ù„ØµÙØ§Øª ÙˆØ§Ù„Ø§ÙØ¹Ø§Ù„ ÙˆØ§Ù„Ø§Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù„ÙŠ ØªØ³Ø§Ø¹Ø¯Ù†Ø§ ÙÙŠ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… <br>
# **Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ StanfordPOSTagger Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**

# In[ ]:


java_path = "C:/Program Files/Java/jre1.8.0_251/bin/java.exe"
os.environ['JAVAHOME'] = java_path


# In[ ]:


jar = "stanford-postagger-full-2018-10-16/stanford-postagger.jar"
model = "stanford-postagger-full-2018-10-16/models/arabic.tagger"
pos_tagger = StanfordPOSTagger(model, jar, encoding = 'utf8')


# In[ ]:


def PartOfSpeech(tokens):
    pos_words = pos_tagger.tag(tokens)
    filtered_tokens = []
    unwanted_tags = {"CC", "NNP", "PRP", 'CD', "IN", 'UH', 'DT'}
    for word in pos_words:
        if word[0]:
            if word[0].split('/')[1] not in unwanted_tags:
                filtered_tokens.append(word[0].split('/')[0])
        elif word[1].split('/')[1] not in unwanted_tags:
            filtered_tokens.append(word[1].split('/')[0])
    return filtered_tokens


# ## 3- Named Entity Recognization (NER) Handling
# Ø¯ÙŠ Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ù„ÙŠ Ø¨ØªÙ…Ø³Ø­ Ø§ÙŠ Ø²ÙŠØ§Ø¯Ø§Øª Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø¹Ù„Ø´Ø§Ù† ØªÙƒÙˆÙ† ÙƒÙ„Ù‡Ø§ ÙƒÙ„Ù…Ø§Øª Ù…ÙˆØ­Ø¯Ù‡ ÙˆÙ…ÙÙŠØ´ ÙØ±Ù‚ ÙÙŠ Ø§Ù„ØªØ´ÙƒÙŠÙ„ <br>
# **Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ transformers -> pipeline Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**

# In[2]:


tokenizer = AutoTokenizer.from_pretrained("models/NER")
model = AutoModelForTokenClassification.from_pretrained("models/NER")
Ner = pipeline("ner", model=model, tokenizer=tokenizer)


# In[ ]:


def NerDetective(sample):
    persons = []
    ner_obj = Ner(sample)
    unwanted_tags = {'B-PRICE', 'I-PRICE', "B-DISEASE", "I-DISEASE", 'B-PERSON', 'I-PERSON'}
    for i in range(len(ner_obj)):
        if ner_obj[i]["entity"] not in unwanted_tags:
            persons.append(ner_obj[i]["word"])
    return persons


# In[ ]:


def CleanNer(sample):
    words = []
    ner_detectived = NerDetective(sample)
    for word in ner_detectived:
        try:
            if word.startswith("##"):
                words[-1] = words[-1] + word.replace("##", "")
            else:
                words.append(word)
        except:
            pass
    return(words)


# In[ ]:


def RemoveNer(tokens, sample):
    cleaned_ner = CleanNer(sample)
    filltered_tokens = [token for token in tokens if token not in cleaned_ner]
    return filltered_tokens


# ## 4- Normalization Function
# Ø¯ÙŠ Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ù„ÙŠ Ø¨ØªÙ…Ø³Ø­ Ø§ÙŠ Ø²ÙŠØ§Ø¯Ø§Øª Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø¹Ù„Ø´Ø§Ù† ØªÙƒÙˆÙ† ÙƒÙ„Ù‡Ø§ ÙƒÙ„Ù…Ø§Øª Ù…ÙˆØ­Ø¯Ù‡ ÙˆÙ…ÙÙŠØ´ ÙØ±Ù‚ ÙÙŠ Ø§Ù„ØªØ´ÙƒÙŠÙ„ <br>
# **Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ re Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**

# In[ ]:


def Normalize(tokens):
    normalized_tokens = []
    for token in tokens:
        token = re.sub("[Ø¥Ø£Ø¢Ø§]", "Ø§", token)
        token = re.sub("Ù‰", "ÙŠ", token)
        token = re.sub("Ø©", "Ù‡", token)
        token = re.sub("[\W\da-zA-Z]", "", token)
        token = re.sub("_", " ", token)
        token = araby.strip_diacritics(token)
        token = araby.strip_tatweel(token)
        if token != "":
            normalized_tokens.append(token)
    return normalized_tokens


# ## 5- Removing Stop Words Function
# ### Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ù‡
# Ù‡Ù†Ø§ Ø¨Ù†Ø§Ø®Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ù‡ ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§Ø³ÙŠØª Ø¨ØªØ§Ø¹Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ù‡ (Ø³ØªÙˆØ¨ ÙˆÙˆØ±Ø¯Ø³)

# In[ ]:


def StopWords():
    sample = open('DataSets\stop_words.txt', 'r', encoding='utf-8')
    sample_Words = str(sample.read())
    stopwords = Tokening(sample_Words)
    return stopwords


# ### Ù…Ø³Ø­ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ù‡ Ù…Ù† Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…Ø¹Ø·Ø§Ù‡
# ÙˆÙ‡Ù†Ø§ Ø¨Ù†Ø´ÙˆÙ Ù„Ùˆ Ø§Ù„ÙƒÙ„Ù…Ø© Ù…ÙˆØ¬ÙˆØ¯Ù‡ ÙÙŠÙ‡Ø§ Ù†Ù…Ø³Ø­Ù‡Ø§

# In[ ]:


def RemoveStopWords(tokens):
    stop_words = StopWords()
    filtered_tokens = [token for token in tokens if token not in stop_words]
    return filtered_tokens


# ## 6- Stemming Function
# Ù‡Ù†Ø§ Ø¨Ù†Ø±Ø¬Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø© Ù„Ø£ØµÙ„Ù‡Ø§ <br>
# **Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ ISRIStemmer Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**

# In[ ]:


def Stemming(tokens):
    stemmer = ISRIStemmer()
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    return stemmed_tokens


# ## 7- Restore tokens to sentence function
# Ù‡Ù†Ø§ Ø¨Ù†Ø±Ø¬Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‰ Ø¬Ù…Ù„<br>

# In[ ]:


def ToSentence(tokens):
    sentence = ' '.join(token for token in tokens)
    return sentence


# ## 8- Generating Word Cloud Figure
# Ù‡Ù†Ø§ Ø¨Ù†Ø¹Ù…Ù„ Ø®Ø±ÙŠØ·Ø© Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø§ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹ ÙÙŠ Ø§Ù„Ø¬Ù…Ù„ Ø§Ùˆ Ø§Ù„Ù…Ù‚Ø§Ù„<br>

# In[ ]:


def wordcloud(df):
    text = " ".join(sentence for sentence in df['cleaned_text'])
    # Reshape and display the Arabic text
    reshaped_text = arabic_reshaper.reshape(text)
    bidi_text = get_display(reshaped_text)
    # Generate the word cloud
    wordcloud = WordCloud(font_path='tahomabd.ttf',
                          width=220, height=220,
                          margin=0, mode='RGBA', background_color=None).generate(bidi_text)
    # Plot the word cloud
    plt.figure(figsize=(3, 3))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.savefig("assets/frame1/wordcloud.png", transparent = True)


# In[ ]:


def SentenceWordCloud(sentence):
    text = sentence
    # Reshape and display the Arabic text
    reshaped_text = arabic_reshaper.reshape(text)
    bidi_text = get_display(reshaped_text)
    # Generate the word cloud
    wordcloud = WordCloud(font_path='tahomabd.ttf',
                          width=1600, height=800,
                          margin=0).generate(bidi_text)
    # Plot the word cloud
    plt.figure(figsize=(20, 10))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.savefig("assets/frame0/wordcloud.png", transparent = True)


# ### Cleaning & Preprocessing Functions

# In[ ]:


def SentencePreProcess(sentence):
    tokens = Tokening(sentence)
    tokens = Normalize(tokens)
    tokens = PartOfSpeech(tokens)
    sentence = ToSentence(tokens)
    tokens = RemoveNer(tokens, sentence)
    sentence = Stemming(tokens)
    sentence = RemoveStopWords(sentence)
    sentence = ToSentence(sentence)
    return sentence


# In[ ]:


def PreProcess(location):
    df = pd.read_csv(location, encoding="utf-8")
    df.columns = ['text']
    print("PreProcessing Started!")
    start_time = time.perf_counter()
    df['cleaned_text'] = df['text'].apply(Tokening)
    end_time = time.perf_counter()
    print("Tokenization complete!", end_time - start_time)
    start_time = time.perf_counter()
    df['cleaned_text'] = df['cleaned_text'].apply(Normalize)
    end_time = time.perf_counter()
    print("Normalization complete!", end_time - start_time)
    start_time = time.perf_counter()
    df['cleaned_text'] = df['cleaned_text'].apply(PartOfSpeech)
    end_time = time.perf_counter()
    print("POS filtering complete!", end_time - start_time)
    df['preprocessed_text'] = df['cleaned_text'].apply(ToSentence)
    start_time = time.perf_counter()
    df['cleaned_text'] = df.apply(lambda x: RemoveNer(x['cleaned_text'], x['preprocessed_text']), axis=1)
    end_time = time.perf_counter()
    print("NER filtering complete!", end_time - start_time)
    start_time = time.perf_counter()
    df['preprocessed_text'] = df['cleaned_text'].apply(Stemming)
    end_time = time.perf_counter()
    print("Stemming complete!", end_time - start_time)
    start_time = time.perf_counter()
    df['preprocessed_text'] = df['preprocessed_text'].apply(RemoveStopWords)
    end_time = time.perf_counter()
    print("Stopword removal complete!", end_time - start_time)
    df['cleaned_text'] = df['cleaned_text'].apply(ToSentence)
    df['preprocessed_text'] = df['preprocessed_text'].apply(ToSentence)
    df.dropna(inplace=True)
    df.drop_duplicates(inplace=True)
    df.reset_index(inplace=True, drop=True)
    return df

