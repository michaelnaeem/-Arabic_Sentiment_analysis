{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fda360",
   "metadata": {},
   "source": [
    "# Preprocessing Stage\n",
    "## Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
    "Ù‡Ù†Ø§ Ù‡Ù†Ø³ØªØ¯Ø¹ÙŠ ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„ÙŠ Ø¨Ù†Ø­ØªØ§Ø¬Ù‡Ø§ Ø®Ù„Ø§Ù„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ec45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pyarabic import araby\n",
    "import os\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "import re\n",
    "from nltk import ISRIStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28483bf4",
   "metadata": {},
   "source": [
    "## 1- Tokenization Function\n",
    "Ø¯ÙŠ Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ù„ÙŠ Ø¨ØªÙ‚Ø³Ù… Ø§Ù„Ø¬Ù…Ù„Ø© Ù„ÙƒÙ„Ù…Ø§Øª <br>\n",
    "**Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ araby Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2550fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokening(sample):\n",
    "    tokens = araby.tokenize(sample)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c7534",
   "metadata": {},
   "source": [
    "ğŸ‘‡Ù…Ø«Ø§Ù„: Ù‡Ù†Ø§ Ù…Ø«Ù„Ø§ Ø¨Ù†Ø¯ÙŠÙ„Ù‡ Ø¬Ù…Ù„Ø© ÙˆÙ‡Ùˆ Ø¨ÙŠÙ‚Ø·Ø¹Ù‡Ø§ ÙˆÙŠØ³Ù…ÙŠÙ‡Ø§ ØªÙˆÙƒÙŠÙ†Ø²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f503c",
   "metadata": {},
   "source": [
    "## 2- Part Of Speech Handling\n",
    "Ø¯ÙŠ Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ù„ÙŠ Ø¨ØªØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§ÙŠ Ø§Ø³Ù… Ø¹Ù„Ù… Ø§Ùˆ Ø­Ø±Ù Ø¬Ø± Ø§Ùˆ ÙƒÙ„Ù…Ø© ØºØ±ÙŠØ¨Ù‡ ÙˆØªØ­Ø°ÙÙ‡Ù… Ù…Ù† Ø§Ù„Ø¬Ù…Ù„Ø© ÙˆØªØ®Ù„ÙŠ Ø¨Ø³ Ø§Ù„ØµÙØ§Øª ÙˆØ§Ù„Ø§ÙØ¹Ø§Ù„ ÙˆØ§Ù„Ø§Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù„ÙŠ ØªØ³Ø§Ø¹Ø¯Ù†Ø§ ÙÙŠ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… <br>\n",
    "**Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ StanfordPOSTagger Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5672a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_path = \"C:/Program Files/Java/jre1.8.0_251/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a9c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar = \"stanford-postagger-full-2018-10-16/stanford-postagger.jar\"\n",
    "model = \"stanford-postagger-full-2018-10-16/models/arabic.tagger\"\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PartOfSpeech(tokens):\n",
    "    pos_words = pos_tagger.tag(tokens)\n",
    "    filtered_tokens = []\n",
    "    unwanted_tags = {\"CC\", \"NNP\", \"PRP\", 'CD', \"IN\", 'UH', 'DT'}\n",
    "    for word in pos_words:\n",
    "        if word[0]:\n",
    "            if word[0].split('/')[1] not in unwanted_tags:\n",
    "                filtered_tokens.append(word[0].split('/')[0])\n",
    "        elif word[1].split('/')[1] not in unwanted_tags:\n",
    "            filtered_tokens.append(word[1].split('/')[0])\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724516b8",
   "metadata": {},
   "source": [
    "## 3- Named Entity Recognization (NER) Handling\n",
    "Ø¯ÙŠ Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ù„ÙŠ Ø¨ØªÙ…Ø³Ø­ Ø§ÙŠ Ø²ÙŠØ§Ø¯Ø§Øª Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø¹Ù„Ø´Ø§Ù† ØªÙƒÙˆÙ† ÙƒÙ„Ù‡Ø§ ÙƒÙ„Ù…Ø§Øª Ù…ÙˆØ­Ø¯Ù‡ ÙˆÙ…ÙÙŠØ´ ÙØ±Ù‚ ÙÙŠ Ø§Ù„ØªØ´ÙƒÙŠÙ„ <br>\n",
    "**Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ transformers -> pipeline Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daafec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"models/NER\")\n",
    "Ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c35ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NerDetective(sample):\n",
    "    persons = []\n",
    "    ner_obj = Ner(sample)\n",
    "    unwanted_tags = {'B-PRICE', 'I-PRICE', \"B-DISEASE\", \"I-DISEASE\", 'B-PERSON', 'I-PERSON'}\n",
    "    for i in range(len(ner_obj)):\n",
    "        if ner_obj[i][\"entity\"] not in unwanted_tags:\n",
    "            persons.append(ner_obj[i][\"word\"])\n",
    "    return persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanNer(sample):\n",
    "    words = []\n",
    "    ner_detectived = NerDetective(sample)\n",
    "    for word in ner_detectived:\n",
    "        try:\n",
    "            if word.startswith(\"##\"):\n",
    "                words[-1] = words[-1] + word.replace(\"##\", \"\")\n",
    "            else:\n",
    "                words.append(word)\n",
    "        except:\n",
    "            pass\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f821ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNer(tokens, sample):\n",
    "    cleaned_ner = CleanNer(sample)\n",
    "    filltered_tokens = [token for token in tokens if token not in cleaned_ner]\n",
    "    return filltered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061d320c",
   "metadata": {},
   "source": [
    "## 4- Normalization Function\n",
    "Ø¯ÙŠ Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ù„ÙŠ Ø¨ØªÙ…Ø³Ø­ Ø§ÙŠ Ø²ÙŠØ§Ø¯Ø§Øª Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø¹Ù„Ø´Ø§Ù† ØªÙƒÙˆÙ† ÙƒÙ„Ù‡Ø§ ÙƒÙ„Ù…Ø§Øª Ù…ÙˆØ­Ø¯Ù‡ ÙˆÙ…ÙÙŠØ´ ÙØ±Ù‚ ÙÙŠ Ø§Ù„ØªØ´ÙƒÙŠÙ„ <br>\n",
    "**Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ re Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b31e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(tokens):\n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        token = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", token)\n",
    "        token = re.sub(\"Ù‰\", \"ÙŠ\", token)\n",
    "        token = re.sub(\"Ø©\", \"Ù‡\", token)\n",
    "        token = re.sub(\"[\\W\\da-zA-Z]\", \"\", token)\n",
    "        token = re.sub(\"_\", \" \", token)\n",
    "        token = araby.strip_diacritics(token)\n",
    "        token = araby.strip_tatweel(token)\n",
    "        if token != \"\":\n",
    "            normalized_tokens.append(token)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b28cab",
   "metadata": {},
   "source": [
    "## 5- Removing Stop Words Function\n",
    "### Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ù‡\n",
    "Ù‡Ù†Ø§ Ø¨Ù†Ø§Ø®Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ù‡ ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§Ø³ÙŠØª Ø¨ØªØ§Ø¹Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ù‡ (Ø³ØªÙˆØ¨ ÙˆÙˆØ±Ø¯Ø³)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StopWords():\n",
    "    sample = open('DataSets\\stop_words.txt', 'r', encoding='utf-8')\n",
    "    sample_Words = str(sample.read())\n",
    "    stopwords = Tokening(sample_Words)\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b3b64",
   "metadata": {},
   "source": [
    "### Ù…Ø³Ø­ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ù‡ Ù…Ù† Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…Ø¹Ø·Ø§Ù‡\n",
    "ÙˆÙ‡Ù†Ø§ Ø¨Ù†Ø´ÙˆÙ Ù„Ùˆ Ø§Ù„ÙƒÙ„Ù…Ø© Ù…ÙˆØ¬ÙˆØ¯Ù‡ ÙÙŠÙ‡Ø§ Ù†Ù…Ø³Ø­Ù‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWords(tokens):\n",
    "    stop_words = StopWords()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d799e8",
   "metadata": {},
   "source": [
    "## 6- Stemming Function\n",
    "Ù‡Ù†Ø§ Ø¨Ù†Ø±Ø¬Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø© Ù„Ø£ØµÙ„Ù‡Ø§ <br>\n",
    "**Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ ISRIStemmer Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stemming(tokens):\n",
    "    stemmer = ISRIStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3864852",
   "metadata": {},
   "source": [
    "## 7- Restore tokens to sentence function\n",
    "Ù‡Ù†Ø§ Ø¨Ù†Ø±Ø¬Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‰ Ø¬Ù…Ù„<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToSentence(tokens):\n",
    "    sentence = ' '.join(token for token in tokens)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7835a0",
   "metadata": {},
   "source": [
    "## 8- Generating Word Cloud Figure\n",
    "Ù‡Ù†Ø§ Ø¨Ù†Ø¹Ù…Ù„ Ø®Ø±ÙŠØ·Ø© Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø§ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹ ÙÙŠ Ø§Ù„Ø¬Ù…Ù„ Ø§Ùˆ Ø§Ù„Ù…Ù‚Ø§Ù„<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff94794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(df):\n",
    "    text = \" \".join(sentence for sentence in df['cleaned_text'])\n",
    "    # Reshape and display the Arabic text\n",
    "    reshaped_text = arabic_reshaper.reshape(text)\n",
    "    bidi_text = get_display(reshaped_text)\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(font_path='tahomabd.ttf',\n",
    "                          width=220, height=220,\n",
    "                          margin=0, mode='RGBA', background_color=None).generate(bidi_text)\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"assets/frame1/wordcloud.png\", transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e866f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentenceWordCloud(sentence):\n",
    "    text = sentence\n",
    "    # Reshape and display the Arabic text\n",
    "    reshaped_text = arabic_reshaper.reshape(text)\n",
    "    bidi_text = get_display(reshaped_text)\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(font_path='tahomabd.ttf',\n",
    "                          width=1600, height=800,\n",
    "                          margin=0).generate(bidi_text)\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"assets/frame0/wordcloud.png\", transparent = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6b836",
   "metadata": {},
   "source": [
    "### Cleaning & Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentencePreProcess(sentence):\n",
    "    tokens = Tokening(sentence)\n",
    "    tokens = Normalize(tokens)\n",
    "    tokens = PartOfSpeech(tokens)\n",
    "    sentence = ToSentence(tokens)\n",
    "    tokens = RemoveNer(tokens, sentence)\n",
    "    sentence = Stemming(tokens)\n",
    "    sentence = RemoveStopWords(sentence)\n",
    "    sentence = ToSentence(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77bdf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def PreProcess(location):\n",
    "    df = pd.read_csv(location, encoding=\"utf-8\")\n",
    "    df.columns = ['text']\n",
    "    print(\"PreProcessing Started!\")\n",
    "    start_time = time.perf_counter()\n",
    "    df['cleaned_text'] = df['text'].apply(Tokening)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Tokenization complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(Normalize)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Normalization complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(PartOfSpeech)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"POS filtering complete!\", end_time - start_time)\n",
    "    df['preprocessed_text'] = df['cleaned_text'].apply(ToSentence)\n",
    "    start_time = time.perf_counter()\n",
    "    df['cleaned_text'] = df.apply(lambda x: RemoveNer(x['cleaned_text'], x['preprocessed_text']), axis=1)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"NER filtering complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['preprocessed_text'] = df['cleaned_text'].apply(Stemming)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Stemming complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['preprocessed_text'] = df['preprocessed_text'].apply(RemoveStopWords)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Stopword removal complete!\", end_time - start_time)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(ToSentence)\n",
    "    df['preprocessed_text'] = df['preprocessed_text'].apply(ToSentence)\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
