{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fda360",
   "metadata": {},
   "source": [
    "# Preprocessing Stage\n",
    "## استدعاء المكتبات\n",
    "هنا هنستدعي كل المكتبات اللي بنحتاجها خلال المشروع"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ec45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pyarabic import araby\n",
    "import os\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "import re\n",
    "from nltk import ISRIStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28483bf4",
   "metadata": {},
   "source": [
    "## 1- Tokenization Function\n",
    "دي الفانكشن اللي بتقسم الجملة لكلمات <br>\n",
    "**اللي استدعيناها فوق araby لاحظ استخدام مكتبة**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2550fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokening(sample):\n",
    "    tokens = araby.tokenize(sample)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c7534",
   "metadata": {},
   "source": [
    "👇مثال: هنا مثلا بنديله جملة وهو بيقطعها ويسميها توكينز"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f503c",
   "metadata": {},
   "source": [
    "## 2- Part Of Speech Handling\n",
    "دي الفانكشن اللي بتتعرف على اي اسم علم او حرف جر او كلمة غريبه وتحذفهم من الجملة وتخلي بس الصفات والافعال والاجزاء اللي تساعدنا في التقييم <br>\n",
    "**اللي استدعيناها فوق لاستدعاء الموديل المطلوب StanfordPOSTagger لاحظ استخدام مكتبة**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5672a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_path = \"C:/Program Files/Java/jre1.8.0_251/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a9c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar = \"stanford-postagger-full-2018-10-16/stanford-postagger.jar\"\n",
    "model = \"stanford-postagger-full-2018-10-16/models/arabic.tagger\"\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PartOfSpeech(tokens):\n",
    "    pos_words = pos_tagger.tag(tokens)\n",
    "    filtered_tokens = []\n",
    "    unwanted_tags = {\"CC\", \"NNP\", \"PRP\", 'CD', \"IN\", 'UH', 'DT'}\n",
    "    for word in pos_words:\n",
    "        if word[0]:\n",
    "            if word[0].split('/')[1] not in unwanted_tags:\n",
    "                filtered_tokens.append(word[0].split('/')[0])\n",
    "        elif word[1].split('/')[1] not in unwanted_tags:\n",
    "            filtered_tokens.append(word[1].split('/')[0])\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724516b8",
   "metadata": {},
   "source": [
    "## 3- Named Entity Recognization (NER) Handling\n",
    "دي الفانكشن اللي بتمسح اي زيادات من الكلمة علشان تكون كلها كلمات موحده ومفيش فرق في التشكيل <br>\n",
    "**اللي استدعيناها فوق لاستدعاء الموديل المطلوب transformers -> pipeline لاحظ استخدام مكتبة**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daafec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"models/NER\")\n",
    "Ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c35ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NerDetective(sample):\n",
    "    persons = []\n",
    "    ner_obj = Ner(sample)\n",
    "    unwanted_tags = {'B-PRICE', 'I-PRICE', \"B-DISEASE\", \"I-DISEASE\", 'B-PERSON', 'I-PERSON'}\n",
    "    for i in range(len(ner_obj)):\n",
    "        if ner_obj[i][\"entity\"] not in unwanted_tags:\n",
    "            persons.append(ner_obj[i][\"word\"])\n",
    "    return persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanNer(sample):\n",
    "    words = []\n",
    "    ner_detectived = NerDetective(sample)\n",
    "    for word in ner_detectived:\n",
    "        try:\n",
    "            if word.startswith(\"##\"):\n",
    "                words[-1] = words[-1] + word.replace(\"##\", \"\")\n",
    "            else:\n",
    "                words.append(word)\n",
    "        except:\n",
    "            pass\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f821ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNer(tokens, sample):\n",
    "    cleaned_ner = CleanNer(sample)\n",
    "    filltered_tokens = [token for token in tokens if token not in cleaned_ner]\n",
    "    return filltered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061d320c",
   "metadata": {},
   "source": [
    "## 4- Normalization Function\n",
    "دي الفانكشن اللي بتمسح اي زيادات من الكلمة علشان تكون كلها كلمات موحده ومفيش فرق في التشكيل <br>\n",
    "**اللي استدعيناها فوق re لاحظ استخدام مكتبة**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b31e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(tokens):\n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        token = re.sub(\"[إأآا]\", \"ا\", token)\n",
    "        token = re.sub(\"ى\", \"ي\", token)\n",
    "        token = re.sub(\"ة\", \"ه\", token)\n",
    "        token = re.sub(\"[\\W\\da-zA-Z]\", \"\", token)\n",
    "        token = re.sub(\"_\", \" \", token)\n",
    "        token = araby.strip_diacritics(token)\n",
    "        token = araby.strip_tatweel(token)\n",
    "        if token != \"\":\n",
    "            normalized_tokens.append(token)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b28cab",
   "metadata": {},
   "source": [
    "## 5- Removing Stop Words Function\n",
    "### استدعاء الكلمات المستبعده\n",
    "هنا بناخد الكلمات الموجوده في الداتاسيت بتاعت الكلمات المستبعده (ستوب ووردس)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StopWords():\n",
    "    sample = open('DataSets\\stop_words.txt', 'r', encoding='utf-8')\n",
    "    sample_Words = str(sample.read())\n",
    "    stopwords = Tokening(sample_Words)\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b3b64",
   "metadata": {},
   "source": [
    "### مسح الكلمات المستبعده من الجملة المعطاه\n",
    "وهنا بنشوف لو الكلمة موجوده فيها نمسحها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWords(tokens):\n",
    "    stop_words = StopWords()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d799e8",
   "metadata": {},
   "source": [
    "## 6- Stemming Function\n",
    "هنا بنرجع الكلمة لأصلها <br>\n",
    "**اللي استدعيناها فوق ISRIStemmer لاحظ استخدام مكتبة**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stemming(tokens):\n",
    "    stemmer = ISRIStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3864852",
   "metadata": {},
   "source": [
    "## 7- Restore tokens to sentence function\n",
    "هنا بنرجع الكلمات الى جمل<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToSentence(tokens):\n",
    "    sentence = ' '.join(token for token in tokens)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7835a0",
   "metadata": {},
   "source": [
    "## 8- Generating Word Cloud Figure\n",
    "هنا بنعمل خريطة للكلمات الاكثر استخداماً في الجمل او المقال<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff94794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(df):\n",
    "    text = \" \".join(sentence for sentence in df['cleaned_text'])\n",
    "    # Reshape and display the Arabic text\n",
    "    reshaped_text = arabic_reshaper.reshape(text)\n",
    "    bidi_text = get_display(reshaped_text)\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(font_path='tahomabd.ttf',\n",
    "                          width=220, height=220,\n",
    "                          margin=0, mode='RGBA', background_color=None).generate(bidi_text)\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"assets/frame1/wordcloud.png\", transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e866f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentenceWordCloud(sentence):\n",
    "    text = sentence\n",
    "    # Reshape and display the Arabic text\n",
    "    reshaped_text = arabic_reshaper.reshape(text)\n",
    "    bidi_text = get_display(reshaped_text)\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(font_path='tahomabd.ttf',\n",
    "                          width=1600, height=800,\n",
    "                          margin=0).generate(bidi_text)\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"assets/frame0/wordcloud.png\", transparent = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6b836",
   "metadata": {},
   "source": [
    "### Cleaning & Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentencePreProcess(sentence):\n",
    "    tokens = Tokening(sentence)\n",
    "    tokens = Normalize(tokens)\n",
    "    tokens = PartOfSpeech(tokens)\n",
    "    sentence = ToSentence(tokens)\n",
    "    tokens = RemoveNer(tokens, sentence)\n",
    "    sentence = Stemming(tokens)\n",
    "    sentence = RemoveStopWords(sentence)\n",
    "    sentence = ToSentence(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77bdf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def PreProcess(location):\n",
    "    df = pd.read_csv(location, encoding=\"utf-8\")\n",
    "    df.columns = ['text']\n",
    "    print(\"PreProcessing Started!\")\n",
    "    start_time = time.perf_counter()\n",
    "    df['cleaned_text'] = df['text'].apply(Tokening)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Tokenization complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(Normalize)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Normalization complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(PartOfSpeech)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"POS filtering complete!\", end_time - start_time)\n",
    "    df['preprocessed_text'] = df['cleaned_text'].apply(ToSentence)\n",
    "    start_time = time.perf_counter()\n",
    "    df['cleaned_text'] = df.apply(lambda x: RemoveNer(x['cleaned_text'], x['preprocessed_text']), axis=1)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"NER filtering complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['preprocessed_text'] = df['cleaned_text'].apply(Stemming)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Stemming complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['preprocessed_text'] = df['preprocessed_text'].apply(RemoveStopWords)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Stopword removal complete!\", end_time - start_time)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(ToSentence)\n",
    "    df['preprocessed_text'] = df['preprocessed_text'].apply(ToSentence)\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
