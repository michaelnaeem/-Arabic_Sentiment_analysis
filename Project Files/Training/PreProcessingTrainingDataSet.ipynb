{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fda360",
   "metadata": {},
   "source": [
    "# Preprocessing Stage\n",
    "## استدعاء المكتبات\n",
    "هنا هنستدعي كل المكتبات اللي بنحتاجها خلال المشروع"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ec45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pyarabic import araby\n",
    "import os\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "import re\n",
    "from nltk import ISRIStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28483bf4",
   "metadata": {},
   "source": [
    "## 1- Tokenization Function\n",
    "دي الفانكشن اللي بتقسم الجملة لكلمات <br>\n",
    "**اللي استدعيناها فوق araby لاحظ استخدام مكتبة**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2550fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokening(sample):\n",
    "    tokens = araby.tokenize(sample)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c7534",
   "metadata": {},
   "source": [
    "👇مثال: هنا مثلا بنديله جملة وهو بيقطعها ويسميها توكينز"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f503c",
   "metadata": {},
   "source": [
    "## 2- Part Of Speech Handling\n",
    "دي الفانكشن اللي بتتعرف على اي اسم علم او حرف جر او كلمة غريبه وتحذفهم من الجملة وتخلي بس الصفات والافعال والاجزاء اللي تساعدنا في التقييم <br>\n",
    "**اللي استدعيناها فوق لاستدعاء الموديل المطلوب StanfordPOSTagger لاحظ استخدام مكتبة**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c5672a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_path = \"C:/Program Files/Java/jre1.8.0_251/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a9c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar = \"stanford-postagger-full-2018-10-16/stanford-postagger.jar\"\n",
    "model = \"stanford-postagger-full-2018-10-16/models/arabic.tagger\"\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6c4077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PartOfSpeech(tokens):\n",
    "    pos_words = pos_tagger.tag(tokens)\n",
    "    filtered_tokens = []\n",
    "    unwanted_tags = {\"CC\", \"NNP\", \"PRP\", 'CD', \"IN\", 'UH', 'DT'}\n",
    "    for word in pos_words:\n",
    "        if word[0]:\n",
    "            if word[0].split('/')[1] not in unwanted_tags:\n",
    "                filtered_tokens.append(word[0].split('/')[0])\n",
    "        elif word[1].split('/')[1] not in unwanted_tags:\n",
    "            filtered_tokens.append(word[1].split('/')[0])\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724516b8",
   "metadata": {},
   "source": [
    "## 3- Named Entity Recognization (NER) Handling\n",
    "دي الفانكشن اللي بتمسح اي زيادات من الكلمة علشان تكون كلها كلمات موحده ومفيش فرق في التشكيل <br>\n",
    "**اللي استدعيناها فوق لاستدعاء الموديل المطلوب transformers -> pipeline لاحظ استخدام مكتبة**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56192e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pipe = pipeline(\"ner\", model=\"hatmimoha/arabic-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339dcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hatmimoha/arabic-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"hatmimoha/arabic-ner\")\n",
    "Ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82c35ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NerDetective(sample):\n",
    "    persons = []\n",
    "    ner_obj = Ner(sample)\n",
    "    unwanted_tags = {'B-PRICE', 'I-PRICE', \"B-DISEASE\", \"I-DISEASE\", 'B-PERSON', 'I-PERSON'}\n",
    "    for i in range(len(ner_obj)):\n",
    "        if ner_obj[i][\"entity\"] not in unwanted_tags:\n",
    "            persons.append(ner_obj[i][\"word\"])\n",
    "    return persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ce0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanNer(sample):\n",
    "    words = []\n",
    "    ner_detectived = NerDetective(sample)\n",
    "    for word in ner_detectived:\n",
    "        try:\n",
    "            if word.startswith(\"##\"):\n",
    "                words[-1] = words[-1] + word.replace(\"##\", \"\")\n",
    "            else:\n",
    "                words.append(word)\n",
    "        except:\n",
    "            pass\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f821ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNer(tokens, sample):\n",
    "    cleaned_ner = CleanNer(sample)\n",
    "    filltered_tokens = [token for token in tokens if token not in cleaned_ner]\n",
    "    return filltered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061d320c",
   "metadata": {},
   "source": [
    "## 4- Normalization Function\n",
    "دي الفانكشن اللي بتمسح اي زيادات من الكلمة علشان تكون كلها كلمات موحده ومفيش فرق في التشكيل <br>\n",
    "**اللي استدعيناها فوق re لاحظ استخدام مكتبة**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b31e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(tokens):\n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        token = re.sub(\"[إأآا]\", \"ا\", token)\n",
    "        token = re.sub(\"ى\", \"ي\", token)\n",
    "#         token = re.sub(\"ة\", \"ه\", token)\n",
    "        token = re.sub(\"[\\W\\da-zA-Z]\", \"\", token)\n",
    "        token = re.sub(\"_\", \" \", token)\n",
    "        token = araby.strip_diacritics(token)\n",
    "        token = araby.strip_tatweel(token)\n",
    "        if token != \"\":\n",
    "            normalized_tokens.append(token)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b28cab",
   "metadata": {},
   "source": [
    "## 5- Removing Stop Words Function\n",
    "### استدعاء الكلمات المستبعده\n",
    "هنا بناخد الكلمات الموجوده في الداتاسيت بتاعت الكلمات المستبعده (ستوب ووردس)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3d2f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StopWords():\n",
    "    sample = open('DataSets\\stop_words.txt', 'r', encoding='utf-8')\n",
    "    sample_Words = str(sample.read())\n",
    "    stopwords = Tokening(sample_Words)\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b3b64",
   "metadata": {},
   "source": [
    "### مسح الكلمات المستبعده من الجملة المعطاه\n",
    "وهنا بنشوف لو الكلمة موجوده فيها نمسحها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33ff7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWords(tokens):\n",
    "    stop_words = StopWords()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d799e8",
   "metadata": {},
   "source": [
    "## 6- Stemming Function\n",
    "هنا بنرجع الكلمة لأصلها <br>\n",
    "**اللي استدعيناها فوق ISRIStemmer لاحظ استخدام مكتبة**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f14babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stemming(tokens):\n",
    "    stemmer = ISRIStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3864852",
   "metadata": {},
   "source": [
    "## 7- Restore tokens to sentence function\n",
    "هنا بنرجع الكلمات الى جمل<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85e6d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToSentence(tokens):\n",
    "    sentence = ' '.join(token for token in tokens)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7835a0",
   "metadata": {},
   "source": [
    "## 8- Generating Word Cloud Figure\n",
    "هنا بنعمل خريطة للكلمات الاكثر استخداماً في الجمل او المقال<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ff94794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenWordCloud(df):\n",
    "    text = \" \".join(sentence for sentence in df['cleaned_text'])\n",
    "    # Reshape and display the Arabic text\n",
    "    reshaped_text = arabic_reshaper.reshape(text)\n",
    "    bidi_text = get_display(reshaped_text)\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(font_path='tahomabd.ttf',\n",
    "                          width=1600, height=800,\n",
    "                          margin=0, background_color=\"white\").generate(bidi_text)\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"fig.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6b836",
   "metadata": {},
   "source": [
    "### Cleaning & Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b08a13ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def PreProcess(df):\n",
    "    print(\"PreProcessing Started!\")\n",
    "    start_time = time.perf_counter()\n",
    "    df['cleaned_text'] = df['text'].apply(Tokening)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Tokenization complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(Normalize)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Normalization complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    # df['cleaned_text'] = df['cleaned_text'].apply(PartOfSpeech)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"POS filtering complete!\", end_time - start_time)\n",
    "    df['preprocessed_text'] = df['cleaned_text'].apply(ToSentence)\n",
    "    start_time = time.perf_counter()\n",
    "    # df['cleaned_text'] = df.apply(lambda x: RemoveNer(x['cleaned_text'], x['preprocessed_text']), axis=1)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"NER filtering complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['preprocessed_text'] = df['cleaned_text'].apply(Stemming)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Stemming complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['preprocessed_text'] = df['preprocessed_text'].apply(RemoveStopWords)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Stopword removal complete!\", end_time - start_time)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(ToSentence)\n",
    "    df['preprocessed_text'] = df['preprocessed_text'].apply(ToSentence)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7438ff",
   "metadata": {},
   "source": [
    "# Reading Sentences stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41665429",
   "metadata": {},
   "source": [
    "ARSAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17713660",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"DataSets\\ArSAS.txt\"\n",
    "# headers = ['text']\n",
    "df_ara = pd.read_csv(file_path, encoding=\"utf-8\", delimiter='\\t')\n",
    "# df.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1885f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ara = df_ara[df_ara['Sentiment_label'] != \"Mixed\"]\n",
    "df_ara = df_ara[df_ara['Sentiment_label_confidence'] >= .5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea108391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ara.drop(columns={'#Tweet_ID', 'Topic', 'Sentiment_label_confidence', 'Speech_act_label', 'Speech_act_label_confidence'},\n",
    "        inplace=True)\n",
    "df_ara.rename(columns={'Tweet_text': 'text', 'Sentiment_label' : 'class'},\n",
    "         inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33589a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ara['class'] = df_ara['class'].replace('Neutral', 'NEU').replace('Negative', 'NEG').replace('Positive', 'POS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7c10326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreProcessing Started!\n",
      "Tokenization complete! 1.312864899984561\n",
      "Normalization complete! 6.24091920000501\n",
      "POS filtering complete! 3.00002284348011e-07\n",
      "NER filtering complete! 9.00006853044033e-07\n",
      "Stemming complete! 3.45221620000666\n",
      "Stopword removal complete! 32.961478999990504\n"
     ]
    }
   ],
   "source": [
    "df_ara = PreProcess(df_ara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b8b5030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ara.dropna(inplace=True)\n",
    "df_ara.drop_duplicates(inplace=True)\n",
    "df_ara.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114dc3fb",
   "metadata": {},
   "source": [
    "TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8f1b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"DataSets\\Tweets.txt\"\n",
    "headers = ['text', 'class']\n",
    "df_tweets = pd.read_csv(file_path, encoding=\"utf-8\", delimiter='\\t')\n",
    "df_tweets.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaddac4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tweets['class'].replace('NEUTRAL', 'NEU', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72ddb9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets[df_tweets['class'] != \"OBJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fb88c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreProcessing Started!\n",
      "Tokenization complete! 0.3343369000358507\n",
      "Normalization complete! 0.8693106999853626\n",
      "POS filtering complete! 7.00005330145359e-07\n",
      "NER filtering complete! 1.00000761449337e-06\n",
      "Stemming complete! 0.5291897000279278\n",
      "Stopword removal complete! 6.650740400014911\n"
     ]
    }
   ],
   "source": [
    "df_tweets = PreProcess(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6220be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.dropna(inplace=True)\n",
    "df_tweets.drop_duplicates(inplace=True)\n",
    "df_tweets.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efafc4",
   "metadata": {},
   "source": [
    "LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdaca2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"DataSets\\TrainingDataSet.csv\"\n",
    "# headers = ['text', 'class']\n",
    "df_local = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "# df_tweets.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50ecd426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreProcessing Started!\n",
      "Tokenization complete! 0.17022289999295026\n",
      "Normalization complete! 0.7585197000298649\n",
      "POS filtering complete! 4.00003045797348e-07\n",
      "NER filtering complete! 1.200009137392044e-06\n",
      "Stemming complete! 0.8485243999748491\n",
      "Stopword removal complete! 22.13287390000187\n"
     ]
    }
   ],
   "source": [
    "df_local = PreProcess(df_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e095c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_local['class']\n",
    "df_local['class'] = df_local['text']\n",
    "df_local['text'] = df_temp\n",
    "df_local.rename(columns={'class':'text','text':'class'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4dd69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_local.dropna(inplace=True)\n",
    "df_local.drop_duplicates(inplace=True)\n",
    "df_local.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0df7d808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14586</th>\n",
       "      <td>قطر مع الاخوان وتركيا الي هم من اشعل فتنة الرب...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>قطر مع الاخوان وتركيا الي هم من اشعل فتنة الرب...</td>\n",
       "      <td>قطر اخو وتر شعل فتن عرب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16373</th>\n",
       "      <td>عقبال ما تحاسبوا الخاين اللي باع الأرض و فرط ف...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>عقبال ما تحاسبوا الخاين اللي باع الارض و فرط ف...</td>\n",
       "      <td>عقبال خين باع ارض و فرط ياه شرب شطر هيف سد الن...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10037</th>\n",
       "      <td>#المقرحي: الضابط المفصول المُتهم في حادث #الوا...</td>\n",
       "      <td>NEU</td>\n",
       "      <td>المقرحي الضابط المفصول المتهم في حادث الواحات ...</td>\n",
       "      <td>قرح ضبط فصل تهم واح صاب بلب فصل</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text class  \\\n",
       "14586  قطر مع الاخوان وتركيا الي هم من اشعل فتنة الرب...   NEG   \n",
       "16373  عقبال ما تحاسبوا الخاين اللي باع الأرض و فرط ف...   NEG   \n",
       "10037  #المقرحي: الضابط المفصول المُتهم في حادث #الوا...   NEU   \n",
       "\n",
       "                                            cleaned_text  \\\n",
       "14586  قطر مع الاخوان وتركيا الي هم من اشعل فتنة الرب...   \n",
       "16373  عقبال ما تحاسبوا الخاين اللي باع الارض و فرط ف...   \n",
       "10037  المقرحي الضابط المفصول المتهم في حادث الواحات ...   \n",
       "\n",
       "                                       preprocessed_text  \n",
       "14586                            قطر اخو وتر شعل فتن عرب  \n",
       "16373  عقبال خين باع ارض و فرط ياه شرب شطر هيف سد الن...  \n",
       "10037                    قرح ضبط فصل تهم واح صاب بلب فصل  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ara.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "358548ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6790</th>\n",
       "      <td>كل سنة وأنت طيب</td>\n",
       "      <td>POS</td>\n",
       "      <td>كل سنة وانت طيب</td>\n",
       "      <td>سنة ونت طيب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7682</th>\n",
       "      <td>متدين</td>\n",
       "      <td>POS</td>\n",
       "      <td>متدين</td>\n",
       "      <td>متد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4406</th>\n",
       "      <td>مش مبسوطه</td>\n",
       "      <td>NEG</td>\n",
       "      <td>مش مبسوطه</td>\n",
       "      <td>مش بسط</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text class     cleaned_text preprocessed_text\n",
       "6790  كل سنة وأنت طيب   POS  كل سنة وانت طيب       سنة ونت طيب\n",
       "7682            متدين   POS            متدين               متد\n",
       "4406        مش مبسوطه   NEG        مش مبسوطه            مش بسط"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_local.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95b152fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1540 _ عقاب المخرب هو القتل عند الحقراء عديمي ...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>عقاب المخرب هو القتل عند الحقراء عديمي الانس...</td>\n",
       "      <td>عقب خرب قتل حقراء سان كفي نطق دنء بتع حيو عق...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>ذو العقل يشقى في النعيم بعقلة و أخو الجهالة في...</td>\n",
       "      <td>NEU</td>\n",
       "      <td>ذو العقل يشقي في النعيم بعقلة و اخو الجهالة في...</td>\n",
       "      <td>عقل يشق نعم عقل و اخو جهل شقو نعم اروع ابيات ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>مشايخ البيادة عكس اسمائهم احمد الطيب اقسي قلب ...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>مشايخ البيادة عكس اسمائهم احمد الطيب اقسي قلب ...</td>\n",
       "      <td>شاخ عكس سمئ حمد طيب اقس قلب سلم الغ صلة حمد كر...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text class  \\\n",
       "393  1540 _ عقاب المخرب هو القتل عند الحقراء عديمي ...   NEG   \n",
       "330  ذو العقل يشقى في النعيم بعقلة و أخو الجهالة في...   NEU   \n",
       "642  مشايخ البيادة عكس اسمائهم احمد الطيب اقسي قلب ...   NEG   \n",
       "\n",
       "                                          cleaned_text  \\\n",
       "393    عقاب المخرب هو القتل عند الحقراء عديمي الانس...   \n",
       "330  ذو العقل يشقي في النعيم بعقلة و اخو الجهالة في...   \n",
       "642  مشايخ البيادة عكس اسمائهم احمد الطيب اقسي قلب ...   \n",
       "\n",
       "                                     preprocessed_text  \n",
       "393    عقب خرب قتل حقراء سان كفي نطق دنء بتع حيو عق...  \n",
       "330  عقل يشق نعم عقل و اخو جهل شقو نعم اروع ابيات ا...  \n",
       "642  شاخ عكس سمئ حمد طيب اقس قلب سلم الغ صلة حمد كر...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f00e72c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_ara,df_local,df_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec5df02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ara.to_csv(\"DataSets\\CleanedTrainingDataSetARA.csv\", encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ce4df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_local.to_csv(\"DataSets\\CleanedTrainingDataSetLOCAL.csv\", encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5d6e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv(\"DataSets\\CleanedTrainingDataSetTWEETS.csv\", encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fdc16119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"DataSets\\CleanedTrainingDataSet.csv\", encoding=\"utf-8\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
