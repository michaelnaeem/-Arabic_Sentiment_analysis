{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fda360",
   "metadata": {},
   "source": [
    "# Preprocessing Stage\n",
    "## Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
    "Ù‡Ù†Ø§ Ù‡Ù†Ø³ØªØ¯Ø¹ÙŠ ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„ÙŠ Ø¨Ù†Ø­ØªØ§Ø¬Ù‡Ø§ Ø®Ù„Ø§Ù„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ec45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pyarabic import araby\n",
    "import os\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "import re\n",
    "from nltk import ISRIStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28483bf4",
   "metadata": {},
   "source": [
    "## 1- Tokenization Function\n",
    "Ø¯ÙŠ Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ù„ÙŠ Ø¨ØªÙ‚Ø³Ù… Ø§Ù„Ø¬Ù…Ù„Ø© Ù„ÙƒÙ„Ù…Ø§Øª <br>\n",
    "**Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ araby Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2550fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokening(sample):\n",
    "    tokens = araby.tokenize(sample)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c7534",
   "metadata": {},
   "source": [
    "ğŸ‘‡Ù…Ø«Ø§Ù„: Ù‡Ù†Ø§ Ù…Ø«Ù„Ø§ Ø¨Ù†Ø¯ÙŠÙ„Ù‡ Ø¬Ù…Ù„Ø© ÙˆÙ‡Ùˆ Ø¨ÙŠÙ‚Ø·Ø¹Ù‡Ø§ ÙˆÙŠØ³Ù…ÙŠÙ‡Ø§ ØªÙˆÙƒÙŠÙ†Ø²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f503c",
   "metadata": {},
   "source": [
    "## 2- Part Of Speech Handling\n",
    "Ø¯ÙŠ Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ù„ÙŠ Ø¨ØªØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§ÙŠ Ø§Ø³Ù… Ø¹Ù„Ù… Ø§Ùˆ Ø­Ø±Ù Ø¬Ø± Ø§Ùˆ ÙƒÙ„Ù…Ø© ØºØ±ÙŠØ¨Ù‡ ÙˆØªØ­Ø°ÙÙ‡Ù… Ù…Ù† Ø§Ù„Ø¬Ù…Ù„Ø© ÙˆØªØ®Ù„ÙŠ Ø¨Ø³ Ø§Ù„ØµÙØ§Øª ÙˆØ§Ù„Ø§ÙØ¹Ø§Ù„ ÙˆØ§Ù„Ø§Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù„ÙŠ ØªØ³Ø§Ø¹Ø¯Ù†Ø§ ÙÙŠ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… <br>\n",
    "**Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ StanfordPOSTagger Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c5672a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_path = \"C:/Program Files/Java/jre1.8.0_251/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a9c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar = \"stanford-postagger-full-2018-10-16/stanford-postagger.jar\"\n",
    "model = \"stanford-postagger-full-2018-10-16/models/arabic.tagger\"\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6c4077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PartOfSpeech(tokens):\n",
    "    pos_words = pos_tagger.tag(tokens)\n",
    "    filtered_tokens = []\n",
    "    unwanted_tags = {\"CC\", \"NNP\", \"PRP\", 'CD', \"IN\", 'UH', 'DT'}\n",
    "    for word in pos_words:\n",
    "        if word[0]:\n",
    "            if word[0].split('/')[1] not in unwanted_tags:\n",
    "                filtered_tokens.append(word[0].split('/')[0])\n",
    "        elif word[1].split('/')[1] not in unwanted_tags:\n",
    "            filtered_tokens.append(word[1].split('/')[0])\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724516b8",
   "metadata": {},
   "source": [
    "## 3- Named Entity Recognization (NER) Handling\n",
    "Ø¯ÙŠ Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ù„ÙŠ Ø¨ØªÙ…Ø³Ø­ Ø§ÙŠ Ø²ÙŠØ§Ø¯Ø§Øª Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø¹Ù„Ø´Ø§Ù† ØªÙƒÙˆÙ† ÙƒÙ„Ù‡Ø§ ÙƒÙ„Ù…Ø§Øª Ù…ÙˆØ­Ø¯Ù‡ ÙˆÙ…ÙÙŠØ´ ÙØ±Ù‚ ÙÙŠ Ø§Ù„ØªØ´ÙƒÙŠÙ„ <br>\n",
    "**Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ transformers -> pipeline Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56192e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pipe = pipeline(\"ner\", model=\"hatmimoha/arabic-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339dcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hatmimoha/arabic-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"hatmimoha/arabic-ner\")\n",
    "Ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82c35ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NerDetective(sample):\n",
    "    persons = []\n",
    "    ner_obj = Ner(sample)\n",
    "    unwanted_tags = {'B-PRICE', 'I-PRICE', \"B-DISEASE\", \"I-DISEASE\", 'B-PERSON', 'I-PERSON'}\n",
    "    for i in range(len(ner_obj)):\n",
    "        if ner_obj[i][\"entity\"] not in unwanted_tags:\n",
    "            persons.append(ner_obj[i][\"word\"])\n",
    "    return persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ce0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanNer(sample):\n",
    "    words = []\n",
    "    ner_detectived = NerDetective(sample)\n",
    "    for word in ner_detectived:\n",
    "        try:\n",
    "            if word.startswith(\"##\"):\n",
    "                words[-1] = words[-1] + word.replace(\"##\", \"\")\n",
    "            else:\n",
    "                words.append(word)\n",
    "        except:\n",
    "            pass\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f821ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNer(tokens, sample):\n",
    "    cleaned_ner = CleanNer(sample)\n",
    "    filltered_tokens = [token for token in tokens if token not in cleaned_ner]\n",
    "    return filltered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061d320c",
   "metadata": {},
   "source": [
    "## 4- Normalization Function\n",
    "Ø¯ÙŠ Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ù„ÙŠ Ø¨ØªÙ…Ø³Ø­ Ø§ÙŠ Ø²ÙŠØ§Ø¯Ø§Øª Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø¹Ù„Ø´Ø§Ù† ØªÙƒÙˆÙ† ÙƒÙ„Ù‡Ø§ ÙƒÙ„Ù…Ø§Øª Ù…ÙˆØ­Ø¯Ù‡ ÙˆÙ…ÙÙŠØ´ ÙØ±Ù‚ ÙÙŠ Ø§Ù„ØªØ´ÙƒÙŠÙ„ <br>\n",
    "**Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ re Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b31e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(tokens):\n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        token = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", token)\n",
    "        token = re.sub(\"Ù‰\", \"ÙŠ\", token)\n",
    "#         token = re.sub(\"Ø©\", \"Ù‡\", token)\n",
    "        token = re.sub(\"[\\W\\da-zA-Z]\", \"\", token)\n",
    "        token = re.sub(\"_\", \" \", token)\n",
    "        token = araby.strip_diacritics(token)\n",
    "        token = araby.strip_tatweel(token)\n",
    "        if token != \"\":\n",
    "            normalized_tokens.append(token)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b28cab",
   "metadata": {},
   "source": [
    "## 5- Removing Stop Words Function\n",
    "### Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ù‡\n",
    "Ù‡Ù†Ø§ Ø¨Ù†Ø§Ø®Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ù‡ ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§Ø³ÙŠØª Ø¨ØªØ§Ø¹Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ù‡ (Ø³ØªÙˆØ¨ ÙˆÙˆØ±Ø¯Ø³)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3d2f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StopWords():\n",
    "    sample = open('DataSets\\stop_words.txt', 'r', encoding='utf-8')\n",
    "    sample_Words = str(sample.read())\n",
    "    stopwords = Tokening(sample_Words)\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b3b64",
   "metadata": {},
   "source": [
    "### Ù…Ø³Ø­ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ù‡ Ù…Ù† Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…Ø¹Ø·Ø§Ù‡\n",
    "ÙˆÙ‡Ù†Ø§ Ø¨Ù†Ø´ÙˆÙ Ù„Ùˆ Ø§Ù„ÙƒÙ„Ù…Ø© Ù…ÙˆØ¬ÙˆØ¯Ù‡ ÙÙŠÙ‡Ø§ Ù†Ù…Ø³Ø­Ù‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33ff7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWords(tokens):\n",
    "    stop_words = StopWords()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d799e8",
   "metadata": {},
   "source": [
    "## 6- Stemming Function\n",
    "Ù‡Ù†Ø§ Ø¨Ù†Ø±Ø¬Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø© Ù„Ø£ØµÙ„Ù‡Ø§ <br>\n",
    "**Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ¯Ø¹ÙŠÙ†Ø§Ù‡Ø§ ÙÙˆÙ‚ ISRIStemmer Ù„Ø§Ø­Ø¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f14babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stemming(tokens):\n",
    "    stemmer = ISRIStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3864852",
   "metadata": {},
   "source": [
    "## 7- Restore tokens to sentence function\n",
    "Ù‡Ù†Ø§ Ø¨Ù†Ø±Ø¬Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‰ Ø¬Ù…Ù„<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85e6d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToSentence(tokens):\n",
    "    sentence = ' '.join(token for token in tokens)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7835a0",
   "metadata": {},
   "source": [
    "## 8- Generating Word Cloud Figure\n",
    "Ù‡Ù†Ø§ Ø¨Ù†Ø¹Ù…Ù„ Ø®Ø±ÙŠØ·Ø© Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø§ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹ ÙÙŠ Ø§Ù„Ø¬Ù…Ù„ Ø§Ùˆ Ø§Ù„Ù…Ù‚Ø§Ù„<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ff94794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenWordCloud(df):\n",
    "    text = \" \".join(sentence for sentence in df['cleaned_text'])\n",
    "    # Reshape and display the Arabic text\n",
    "    reshaped_text = arabic_reshaper.reshape(text)\n",
    "    bidi_text = get_display(reshaped_text)\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(font_path='tahomabd.ttf',\n",
    "                          width=1600, height=800,\n",
    "                          margin=0, background_color=\"white\").generate(bidi_text)\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"fig.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6b836",
   "metadata": {},
   "source": [
    "### Cleaning & Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b08a13ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def PreProcess(df):\n",
    "    print(\"PreProcessing Started!\")\n",
    "    start_time = time.perf_counter()\n",
    "    df['cleaned_text'] = df['text'].apply(Tokening)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Tokenization complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(Normalize)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Normalization complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    # df['cleaned_text'] = df['cleaned_text'].apply(PartOfSpeech)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"POS filtering complete!\", end_time - start_time)\n",
    "    df['preprocessed_text'] = df['cleaned_text'].apply(ToSentence)\n",
    "    start_time = time.perf_counter()\n",
    "    # df['cleaned_text'] = df.apply(lambda x: RemoveNer(x['cleaned_text'], x['preprocessed_text']), axis=1)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"NER filtering complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['preprocessed_text'] = df['cleaned_text'].apply(Stemming)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Stemming complete!\", end_time - start_time)\n",
    "    start_time = time.perf_counter()\n",
    "    df['preprocessed_text'] = df['preprocessed_text'].apply(RemoveStopWords)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Stopword removal complete!\", end_time - start_time)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(ToSentence)\n",
    "    df['preprocessed_text'] = df['preprocessed_text'].apply(ToSentence)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7438ff",
   "metadata": {},
   "source": [
    "# Reading Sentences stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41665429",
   "metadata": {},
   "source": [
    "ARSAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17713660",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"DataSets\\ArSAS.txt\"\n",
    "# headers = ['text']\n",
    "df_ara = pd.read_csv(file_path, encoding=\"utf-8\", delimiter='\\t')\n",
    "# df.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1885f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ara = df_ara[df_ara['Sentiment_label'] != \"Mixed\"]\n",
    "df_ara = df_ara[df_ara['Sentiment_label_confidence'] >= .5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea108391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ara.drop(columns={'#Tweet_ID', 'Topic', 'Sentiment_label_confidence', 'Speech_act_label', 'Speech_act_label_confidence'},\n",
    "        inplace=True)\n",
    "df_ara.rename(columns={'Tweet_text': 'text', 'Sentiment_label' : 'class'},\n",
    "         inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33589a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ara['class'] = df_ara['class'].replace('Neutral', 'NEU').replace('Negative', 'NEG').replace('Positive', 'POS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7c10326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreProcessing Started!\n",
      "Tokenization complete! 1.312864899984561\n",
      "Normalization complete! 6.24091920000501\n",
      "POS filtering complete! 3.00002284348011e-07\n",
      "NER filtering complete! 9.00006853044033e-07\n",
      "Stemming complete! 3.45221620000666\n",
      "Stopword removal complete! 32.961478999990504\n"
     ]
    }
   ],
   "source": [
    "df_ara = PreProcess(df_ara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b8b5030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ara.dropna(inplace=True)\n",
    "df_ara.drop_duplicates(inplace=True)\n",
    "df_ara.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114dc3fb",
   "metadata": {},
   "source": [
    "TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8f1b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"DataSets\\Tweets.txt\"\n",
    "headers = ['text', 'class']\n",
    "df_tweets = pd.read_csv(file_path, encoding=\"utf-8\", delimiter='\\t')\n",
    "df_tweets.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaddac4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tweets['class'].replace('NEUTRAL', 'NEU', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72ddb9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets[df_tweets['class'] != \"OBJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fb88c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreProcessing Started!\n",
      "Tokenization complete! 0.3343369000358507\n",
      "Normalization complete! 0.8693106999853626\n",
      "POS filtering complete! 7.00005330145359e-07\n",
      "NER filtering complete! 1.00000761449337e-06\n",
      "Stemming complete! 0.5291897000279278\n",
      "Stopword removal complete! 6.650740400014911\n"
     ]
    }
   ],
   "source": [
    "df_tweets = PreProcess(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6220be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.dropna(inplace=True)\n",
    "df_tweets.drop_duplicates(inplace=True)\n",
    "df_tweets.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efafc4",
   "metadata": {},
   "source": [
    "LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdaca2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"DataSets\\TrainingDataSet.csv\"\n",
    "# headers = ['text', 'class']\n",
    "df_local = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "# df_tweets.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50ecd426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreProcessing Started!\n",
      "Tokenization complete! 0.17022289999295026\n",
      "Normalization complete! 0.7585197000298649\n",
      "POS filtering complete! 4.00003045797348e-07\n",
      "NER filtering complete! 1.200009137392044e-06\n",
      "Stemming complete! 0.8485243999748491\n",
      "Stopword removal complete! 22.13287390000187\n"
     ]
    }
   ],
   "source": [
    "df_local = PreProcess(df_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e095c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_local['class']\n",
    "df_local['class'] = df_local['text']\n",
    "df_local['text'] = df_temp\n",
    "df_local.rename(columns={'class':'text','text':'class'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4dd69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_local.dropna(inplace=True)\n",
    "df_local.drop_duplicates(inplace=True)\n",
    "df_local.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0df7d808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14586</th>\n",
       "      <td>Ù‚Ø·Ø± Ù…Ø¹ Ø§Ù„Ø§Ø®ÙˆØ§Ù† ÙˆØªØ±ÙƒÙŠØ§ Ø§Ù„ÙŠ Ù‡Ù… Ù…Ù† Ø§Ø´Ø¹Ù„ ÙØªÙ†Ø© Ø§Ù„Ø±Ø¨...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>Ù‚Ø·Ø± Ù…Ø¹ Ø§Ù„Ø§Ø®ÙˆØ§Ù† ÙˆØªØ±ÙƒÙŠØ§ Ø§Ù„ÙŠ Ù‡Ù… Ù…Ù† Ø§Ø´Ø¹Ù„ ÙØªÙ†Ø© Ø§Ù„Ø±Ø¨...</td>\n",
       "      <td>Ù‚Ø·Ø± Ø§Ø®Ùˆ ÙˆØªØ± Ø´Ø¹Ù„ ÙØªÙ† Ø¹Ø±Ø¨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16373</th>\n",
       "      <td>Ø¹Ù‚Ø¨Ø§Ù„ Ù…Ø§ ØªØ­Ø§Ø³Ø¨ÙˆØ§ Ø§Ù„Ø®Ø§ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø¨Ø§Ø¹ Ø§Ù„Ø£Ø±Ø¶ Ùˆ ÙØ±Ø· Ù...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>Ø¹Ù‚Ø¨Ø§Ù„ Ù…Ø§ ØªØ­Ø§Ø³Ø¨ÙˆØ§ Ø§Ù„Ø®Ø§ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø¨Ø§Ø¹ Ø§Ù„Ø§Ø±Ø¶ Ùˆ ÙØ±Ø· Ù...</td>\n",
       "      <td>Ø¹Ù‚Ø¨Ø§Ù„ Ø®ÙŠÙ† Ø¨Ø§Ø¹ Ø§Ø±Ø¶ Ùˆ ÙØ±Ø· ÙŠØ§Ù‡ Ø´Ø±Ø¨ Ø´Ø·Ø± Ù‡ÙŠÙ Ø³Ø¯ Ø§Ù„Ù†...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10037</th>\n",
       "      <td>#Ø§Ù„Ù…Ù‚Ø±Ø­ÙŠ: Ø§Ù„Ø¶Ø§Ø¨Ø· Ø§Ù„Ù…ÙØµÙˆÙ„ Ø§Ù„Ù…ÙØªÙ‡Ù… ÙÙŠ Ø­Ø§Ø¯Ø« #Ø§Ù„ÙˆØ§...</td>\n",
       "      <td>NEU</td>\n",
       "      <td>Ø§Ù„Ù…Ù‚Ø±Ø­ÙŠ Ø§Ù„Ø¶Ø§Ø¨Ø· Ø§Ù„Ù…ÙØµÙˆÙ„ Ø§Ù„Ù…ØªÙ‡Ù… ÙÙŠ Ø­Ø§Ø¯Ø« Ø§Ù„ÙˆØ§Ø­Ø§Øª ...</td>\n",
       "      <td>Ù‚Ø±Ø­ Ø¶Ø¨Ø· ÙØµÙ„ ØªÙ‡Ù… ÙˆØ§Ø­ ØµØ§Ø¨ Ø¨Ù„Ø¨ ÙØµÙ„</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text class  \\\n",
       "14586  Ù‚Ø·Ø± Ù…Ø¹ Ø§Ù„Ø§Ø®ÙˆØ§Ù† ÙˆØªØ±ÙƒÙŠØ§ Ø§Ù„ÙŠ Ù‡Ù… Ù…Ù† Ø§Ø´Ø¹Ù„ ÙØªÙ†Ø© Ø§Ù„Ø±Ø¨...   NEG   \n",
       "16373  Ø¹Ù‚Ø¨Ø§Ù„ Ù…Ø§ ØªØ­Ø§Ø³Ø¨ÙˆØ§ Ø§Ù„Ø®Ø§ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø¨Ø§Ø¹ Ø§Ù„Ø£Ø±Ø¶ Ùˆ ÙØ±Ø· Ù...   NEG   \n",
       "10037  #Ø§Ù„Ù…Ù‚Ø±Ø­ÙŠ: Ø§Ù„Ø¶Ø§Ø¨Ø· Ø§Ù„Ù…ÙØµÙˆÙ„ Ø§Ù„Ù…ÙØªÙ‡Ù… ÙÙŠ Ø­Ø§Ø¯Ø« #Ø§Ù„ÙˆØ§...   NEU   \n",
       "\n",
       "                                            cleaned_text  \\\n",
       "14586  Ù‚Ø·Ø± Ù…Ø¹ Ø§Ù„Ø§Ø®ÙˆØ§Ù† ÙˆØªØ±ÙƒÙŠØ§ Ø§Ù„ÙŠ Ù‡Ù… Ù…Ù† Ø§Ø´Ø¹Ù„ ÙØªÙ†Ø© Ø§Ù„Ø±Ø¨...   \n",
       "16373  Ø¹Ù‚Ø¨Ø§Ù„ Ù…Ø§ ØªØ­Ø§Ø³Ø¨ÙˆØ§ Ø§Ù„Ø®Ø§ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø¨Ø§Ø¹ Ø§Ù„Ø§Ø±Ø¶ Ùˆ ÙØ±Ø· Ù...   \n",
       "10037  Ø§Ù„Ù…Ù‚Ø±Ø­ÙŠ Ø§Ù„Ø¶Ø§Ø¨Ø· Ø§Ù„Ù…ÙØµÙˆÙ„ Ø§Ù„Ù…ØªÙ‡Ù… ÙÙŠ Ø­Ø§Ø¯Ø« Ø§Ù„ÙˆØ§Ø­Ø§Øª ...   \n",
       "\n",
       "                                       preprocessed_text  \n",
       "14586                            Ù‚Ø·Ø± Ø§Ø®Ùˆ ÙˆØªØ± Ø´Ø¹Ù„ ÙØªÙ† Ø¹Ø±Ø¨  \n",
       "16373  Ø¹Ù‚Ø¨Ø§Ù„ Ø®ÙŠÙ† Ø¨Ø§Ø¹ Ø§Ø±Ø¶ Ùˆ ÙØ±Ø· ÙŠØ§Ù‡ Ø´Ø±Ø¨ Ø´Ø·Ø± Ù‡ÙŠÙ Ø³Ø¯ Ø§Ù„Ù†...  \n",
       "10037                    Ù‚Ø±Ø­ Ø¶Ø¨Ø· ÙØµÙ„ ØªÙ‡Ù… ÙˆØ§Ø­ ØµØ§Ø¨ Ø¨Ù„Ø¨ ÙØµÙ„  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ara.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "358548ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6790</th>\n",
       "      <td>ÙƒÙ„ Ø³Ù†Ø© ÙˆØ£Ù†Øª Ø·ÙŠØ¨</td>\n",
       "      <td>POS</td>\n",
       "      <td>ÙƒÙ„ Ø³Ù†Ø© ÙˆØ§Ù†Øª Ø·ÙŠØ¨</td>\n",
       "      <td>Ø³Ù†Ø© ÙˆÙ†Øª Ø·ÙŠØ¨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7682</th>\n",
       "      <td>Ù…ØªØ¯ÙŠÙ†</td>\n",
       "      <td>POS</td>\n",
       "      <td>Ù…ØªØ¯ÙŠÙ†</td>\n",
       "      <td>Ù…ØªØ¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4406</th>\n",
       "      <td>Ù…Ø´ Ù…Ø¨Ø³ÙˆØ·Ù‡</td>\n",
       "      <td>NEG</td>\n",
       "      <td>Ù…Ø´ Ù…Ø¨Ø³ÙˆØ·Ù‡</td>\n",
       "      <td>Ù…Ø´ Ø¨Ø³Ø·</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text class     cleaned_text preprocessed_text\n",
       "6790  ÙƒÙ„ Ø³Ù†Ø© ÙˆØ£Ù†Øª Ø·ÙŠØ¨   POS  ÙƒÙ„ Ø³Ù†Ø© ÙˆØ§Ù†Øª Ø·ÙŠØ¨       Ø³Ù†Ø© ÙˆÙ†Øª Ø·ÙŠØ¨\n",
       "7682            Ù…ØªØ¯ÙŠÙ†   POS            Ù…ØªØ¯ÙŠÙ†               Ù…ØªØ¯\n",
       "4406        Ù…Ø´ Ù…Ø¨Ø³ÙˆØ·Ù‡   NEG        Ù…Ø´ Ù…Ø¨Ø³ÙˆØ·Ù‡            Ù…Ø´ Ø¨Ø³Ø·"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_local.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95b152fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1540 _ Ø¹Ù‚Ø§Ø¨ Ø§Ù„Ù…Ø®Ø±Ø¨ Ù‡Ùˆ Ø§Ù„Ù‚ØªÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ù‚Ø±Ø§Ø¡ Ø¹Ø¯ÙŠÙ…ÙŠ ...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>Ø¹Ù‚Ø§Ø¨ Ø§Ù„Ù…Ø®Ø±Ø¨ Ù‡Ùˆ Ø§Ù„Ù‚ØªÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ù‚Ø±Ø§Ø¡ Ø¹Ø¯ÙŠÙ…ÙŠ Ø§Ù„Ø§Ù†Ø³...</td>\n",
       "      <td>Ø¹Ù‚Ø¨ Ø®Ø±Ø¨ Ù‚ØªÙ„ Ø­Ù‚Ø±Ø§Ø¡ Ø³Ø§Ù† ÙƒÙÙŠ Ù†Ø·Ù‚ Ø¯Ù†Ø¡ Ø¨ØªØ¹ Ø­ÙŠÙˆ Ø¹Ù‚...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Ø°Ùˆ Ø§Ù„Ø¹Ù‚Ù„ ÙŠØ´Ù‚Ù‰ ÙÙŠ Ø§Ù„Ù†Ø¹ÙŠÙ… Ø¨Ø¹Ù‚Ù„Ø© Ùˆ Ø£Ø®Ùˆ Ø§Ù„Ø¬Ù‡Ø§Ù„Ø© ÙÙŠ...</td>\n",
       "      <td>NEU</td>\n",
       "      <td>Ø°Ùˆ Ø§Ù„Ø¹Ù‚Ù„ ÙŠØ´Ù‚ÙŠ ÙÙŠ Ø§Ù„Ù†Ø¹ÙŠÙ… Ø¨Ø¹Ù‚Ù„Ø© Ùˆ Ø§Ø®Ùˆ Ø§Ù„Ø¬Ù‡Ø§Ù„Ø© ÙÙŠ...</td>\n",
       "      <td>Ø¹Ù‚Ù„ ÙŠØ´Ù‚ Ù†Ø¹Ù… Ø¹Ù‚Ù„ Ùˆ Ø§Ø®Ùˆ Ø¬Ù‡Ù„ Ø´Ù‚Ùˆ Ù†Ø¹Ù… Ø§Ø±ÙˆØ¹ Ø§Ø¨ÙŠØ§Øª Ø§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>Ù…Ø´Ø§ÙŠØ® Ø§Ù„Ø¨ÙŠØ§Ø¯Ø© Ø¹ÙƒØ³ Ø§Ø³Ù…Ø§Ø¦Ù‡Ù… Ø§Ø­Ù…Ø¯ Ø§Ù„Ø·ÙŠØ¨ Ø§Ù‚Ø³ÙŠ Ù‚Ù„Ø¨ ...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>Ù…Ø´Ø§ÙŠØ® Ø§Ù„Ø¨ÙŠØ§Ø¯Ø© Ø¹ÙƒØ³ Ø§Ø³Ù…Ø§Ø¦Ù‡Ù… Ø§Ø­Ù…Ø¯ Ø§Ù„Ø·ÙŠØ¨ Ø§Ù‚Ø³ÙŠ Ù‚Ù„Ø¨ ...</td>\n",
       "      <td>Ø´Ø§Ø® Ø¹ÙƒØ³ Ø³Ù…Ø¦ Ø­Ù…Ø¯ Ø·ÙŠØ¨ Ø§Ù‚Ø³ Ù‚Ù„Ø¨ Ø³Ù„Ù… Ø§Ù„Øº ØµÙ„Ø© Ø­Ù…Ø¯ ÙƒØ±...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text class  \\\n",
       "393  1540 _ Ø¹Ù‚Ø§Ø¨ Ø§Ù„Ù…Ø®Ø±Ø¨ Ù‡Ùˆ Ø§Ù„Ù‚ØªÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ù‚Ø±Ø§Ø¡ Ø¹Ø¯ÙŠÙ…ÙŠ ...   NEG   \n",
       "330  Ø°Ùˆ Ø§Ù„Ø¹Ù‚Ù„ ÙŠØ´Ù‚Ù‰ ÙÙŠ Ø§Ù„Ù†Ø¹ÙŠÙ… Ø¨Ø¹Ù‚Ù„Ø© Ùˆ Ø£Ø®Ùˆ Ø§Ù„Ø¬Ù‡Ø§Ù„Ø© ÙÙŠ...   NEU   \n",
       "642  Ù…Ø´Ø§ÙŠØ® Ø§Ù„Ø¨ÙŠØ§Ø¯Ø© Ø¹ÙƒØ³ Ø§Ø³Ù…Ø§Ø¦Ù‡Ù… Ø§Ø­Ù…Ø¯ Ø§Ù„Ø·ÙŠØ¨ Ø§Ù‚Ø³ÙŠ Ù‚Ù„Ø¨ ...   NEG   \n",
       "\n",
       "                                          cleaned_text  \\\n",
       "393    Ø¹Ù‚Ø§Ø¨ Ø§Ù„Ù…Ø®Ø±Ø¨ Ù‡Ùˆ Ø§Ù„Ù‚ØªÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ù‚Ø±Ø§Ø¡ Ø¹Ø¯ÙŠÙ…ÙŠ Ø§Ù„Ø§Ù†Ø³...   \n",
       "330  Ø°Ùˆ Ø§Ù„Ø¹Ù‚Ù„ ÙŠØ´Ù‚ÙŠ ÙÙŠ Ø§Ù„Ù†Ø¹ÙŠÙ… Ø¨Ø¹Ù‚Ù„Ø© Ùˆ Ø§Ø®Ùˆ Ø§Ù„Ø¬Ù‡Ø§Ù„Ø© ÙÙŠ...   \n",
       "642  Ù…Ø´Ø§ÙŠØ® Ø§Ù„Ø¨ÙŠØ§Ø¯Ø© Ø¹ÙƒØ³ Ø§Ø³Ù…Ø§Ø¦Ù‡Ù… Ø§Ø­Ù…Ø¯ Ø§Ù„Ø·ÙŠØ¨ Ø§Ù‚Ø³ÙŠ Ù‚Ù„Ø¨ ...   \n",
       "\n",
       "                                     preprocessed_text  \n",
       "393    Ø¹Ù‚Ø¨ Ø®Ø±Ø¨ Ù‚ØªÙ„ Ø­Ù‚Ø±Ø§Ø¡ Ø³Ø§Ù† ÙƒÙÙŠ Ù†Ø·Ù‚ Ø¯Ù†Ø¡ Ø¨ØªØ¹ Ø­ÙŠÙˆ Ø¹Ù‚...  \n",
       "330  Ø¹Ù‚Ù„ ÙŠØ´Ù‚ Ù†Ø¹Ù… Ø¹Ù‚Ù„ Ùˆ Ø§Ø®Ùˆ Ø¬Ù‡Ù„ Ø´Ù‚Ùˆ Ù†Ø¹Ù… Ø§Ø±ÙˆØ¹ Ø§Ø¨ÙŠØ§Øª Ø§...  \n",
       "642  Ø´Ø§Ø® Ø¹ÙƒØ³ Ø³Ù…Ø¦ Ø­Ù…Ø¯ Ø·ÙŠØ¨ Ø§Ù‚Ø³ Ù‚Ù„Ø¨ Ø³Ù„Ù… Ø§Ù„Øº ØµÙ„Ø© Ø­Ù…Ø¯ ÙƒØ±...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f00e72c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_ara,df_local,df_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec5df02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ara.to_csv(\"DataSets\\CleanedTrainingDataSetARA.csv\", encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ce4df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_local.to_csv(\"DataSets\\CleanedTrainingDataSetLOCAL.csv\", encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5d6e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv(\"DataSets\\CleanedTrainingDataSetTWEETS.csv\", encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fdc16119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"DataSets\\CleanedTrainingDataSet.csv\", encoding=\"utf-8\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
